#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Backs up installers, extras, etc. for all your GOG.com purchases.
#
# Copyright (C) 2011, 2012  Evan Powers
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Usage: %s [--version] [--help] <command> [<args>]

Possible commands:

  login <email> [<password>]
    Login to GOG.com using the supplied email and password.
    If password is not supplied, user is promted after start.

  manifest
    Create a manifest describing all games and extras owned by the
    current account and including all available verification metadata.

  list
    Print a summary of the current manifest.

  compare
  update
    Verify downloaded files by comparing them against the manifest,
    and print a report on what must be downloaded. 'compare' validates
    everything; 'update' only validates new files.

  fetch [--all|-a]
    Download any missing, incomplete, or corrupted files (after an
    implicit 'update'). If --all is specified, dowloads every game, else
    prompts for comfirmation.
"""

__author__ = 'Evan Powers'
__version__ = '1.0'
__url__ = 'https://github.com/evanpowers/gog-backup'

import sys, os, codecs, contextlib, pprint, hashlib, locale, zipfile
import cookielib, urllib, urllib2, urlparse
import xml.etree.ElementTree
import threading, Queue, time
import json
from getpass import getpass

import html5lib  # http://code.google.com/p/html5lib/

BACKUPINTO = '.'
CONCURRENCY = 6
MAXPAGELIMIT = 100
BREADTHFIRST = False
FETCHCOVERS = False
DEBUGPROXY = False
PATHMAP = '.gog.pathmap.txt'
COOKIES = '.gog.cookies'
MANIFEST = '.gog.games.py'
VALIDATED = '.gog.valid.py'

LOGIN = 'https://secure.gog.com/login'
AJAX_URL = 'http://www.gog.com/user/ajax/'
ACCOUNT_AJAX_URL = 'https://secure.gog.com/en/account/ajax'
SHELF = 'https://secure.gog.com/account/games/shelf'
# match games to covers on the shelf page
LIST = 'https://secure.gog.com/account/games/list'
THUMB = 'http://www.gog.com'

pathmap = {}
cookiejar = cookielib.LWPCookieJar(COOKIES)
cookieproc = urllib2.HTTPCookieProcessor(cookiejar)
if DEBUGPROXY:
    proxy_handler = urllib2.ProxyHandler({'https': 'https://127.0.0.1:8888'})
    opener = urllib2.build_opener(proxy_handler, cookieproc, urllib2.HTTPHandler(debuglevel=1))
else:
    opener = urllib2.build_opener(cookieproc)
treebuilder = html5lib.treebuilders.getTreeBuilder('etree')
parser = html5lib.HTMLParser(tree=treebuilder, namespaceHTMLElements=False)
sys.stdout = codecs.getwriter(locale.getpreferredencoding())(sys.stdout)

useragent = 'gog-backup/%s (%s)' % (__version__, __url__)
opener.addheaders = [('User-agent', useragent)]

class AttrDict(dict):
    def __init__(self, **kw):
        self.update(kw)
    def __getattr__(self, key):
        return self[key]
    def __setattr__(self, key, val):
        self[key] = val

def request(url, args=None, range=None, hide=False):
    if not hide:
        print '>', url,
        sys.stdout.flush()
    if args is not None:
        args = urllib.urlencode(args)
    req = urllib2.Request(url, data=args)
    if range is not None:
        req.add_header('Range', 'bytes=%d-%d' % range)
    page = opener.open(req)
    if not hide:
        print '[%d]' % page.getcode()
    assert 200 <= page.getcode() < 300
    if page.geturl().endswith('/login') and url != LOGIN:
        # must have been redirected to a login page
        raise RuntimeError('login required')
    return contextlib.closing(page)

def load_attrdicts(fn):
    try:
        with open(fn, 'rU') as r:
            ad = r.read().replace('{', 'AttrDict(**{').replace('}', '})')
        return eval(ad)
    except IOError:
        return AttrDict()

def locate(g, f=None, is_extra=False):
    filename = f.name if f is not None else ""
    path = os.path.join(pathmap.get(g.key, g.title),
                         (is_extra and "extra" or ""), filename)
    # what does it do?
    if not os.path.isfile(path) and os.path.isfile(filename):
        return filename
    return path

def md5map(f, path):
    H = hashlib.md5()
    part = []
    with open(path, 'rb') as r:
        for start, end, md5 in f.part:
            h = hashlib.md5()
            n = end + 1 - start
            r.seek(start)
            p = r.read(n)
            H.update(p)
            h.update(p)
            if len(p) < n:
                # assume the chunk is merely incomplete
                part.append((start + len(p), end))
            elif md5 != h.hexdigest():
                part.append((start, end))
    eq = f.md5 == H.hexdigest()
    if (eq and part) or (not eq and not part):
        # happens with "The Witcher: Enhanced Edition"
        print 'WARNING: error in chunk XML for %s?!' % (f.name,)
        print '    GOG.com must have a bug somewhere.'
        print '    %s != %s' % (f.md5, H.hexdigest())
        print '    bad chunks: %r' % (part,)
    return part

def zipmap(f, path):
    print '&', path
    part = []
    with zipfile.ZipFile(path, 'r') as r:
        if r.testzip() is not None:
            part = [(0, f.size-1)]
    return part

def compare(valid):
    # compare what we have to the manifest
    games = load_attrdicts(MANIFEST)
    missing, corrupt = [], []
    for g in games:
        for f in g.setup + g.extra:
            k = g.key, f.name
            u = (f.size, f.md5) if hasattr(f, 'md5') else f.size
            v = valid.get(k)
            if not v or v.uniq != u:
                v = AttrDict(key=k, uniq=u, need=[(0, f.size-1)])
                valid[k] = v
            elif not v.need:
                continue  # cache says we have this file

            # try to find a local copy
            path = locate(g, f)
            if not os.path.isfile(path):
                missing.append((g, f))
                continue

            # validate the file we have
            sz = os.path.getsize(path)
            if hasattr(f, 'part'):
                v.need = md5map(f, path)
            elif sz < f.size:
                # assume partial content isn't corrupted...
                v.need = [(sz, f.size-1)]
            elif sz > f.size:
                # ...but extra means total corruption
                v.need = [(0, f.size-1)]
                v.drop = True
            elif path.endswith('.zip'):
                # don't need metadata to validate a .zip
                v.need = zipmap(f, path)
            else:
                v.need = []
            if v.need:
                corrupt.append((g, f))

    # write the validity cache to disk
    with open(VALIDATED, 'w') as w:
        print >>w, '# %d files' % len(valid)
        pprint.pprint([valid[k] for k in sorted(valid)], width=100, stream=w)

    return valid, games, missing, corrupt

def megs(b):
    return '%.1fM' % (b / 1000.0**2)

def needed(valid, games, missing, corrupt):
    # print a simple report
    def need(g, f):
        v = valid[(g.key, f.name)]
        return sum(e + 1 - s for s, e in v.need)
    for txt, lst in (('~corrupt', corrupt), ('-missing', missing)):
        mb = megs(sum(need(g, f) for g, f in lst))
        print '%d files are %s (%s):' % (len(lst), txt[1:], mb)
        for g, f in lst:
            pcent = 100.0 * need(g, f) / f.size
            print txt[0], f.name,
            if pcent < 100:
                print '\t(%s, %d%%)' % (megs(need(g, f)), pcent),
            print

def open_notrunc(name, bufsize=4*1024):
    # 'w+' includes O_TRUNC, 'r+' lacks O_CREAT; so, roll my own
    flags = os.O_WRONLY | os.O_CREAT
    if hasattr(os, "O_BINARY"):
        # exists only on windows
        flags |= os.O_BINARY
    fd = os.open(name, flags, 0666)
    return os.fdopen(fd, 'wb', bufsize)

def cmd_login(email, passwd=None):
    if passwd is None:
        passwd = getpass("Password: ")
    # Reset cookiejar
    cookiejar.clear()
    with request(AJAX_URL, args={'a': 'get'}) as json_data:
        data = json.load(json_data)
        buk = data['buk']

    with request(LOGIN, args={'log_email': email,
                              'log_password': passwd,
                              'redirectOk': '/en/',
                              'unlockSettings': '1',
                              'buk': buk,
                                }) as page:
        pass

    guc_al = None
    for c in cookiejar:
        if c.name == 'guc_al':
            guc_al = c
            break
    if not guc_al or guc_al.value == '0':
        raise RuntimeError("Login failed")

    cookiejar.save()

def request_page(name, page_num):
    with request(ACCOUNT_AJAX_URL, args={'a': name, 'p': page_num, 's': 'date_purchased',
                                         'q': '', 't': 0, 'h': 0}) as data_request:
        page_data = json.load(data_request)
        if page_data['result'] != 'ok':
            raise ValueError
        return page_data['html'], page_data['count']

def request_all_pages(name):
    html = '<html>\n<head></head>\n<body>\n'

    i = 1
    total_count = 0
    while True:
        if i > MAXPAGELIMIT:
            print 'Max page limit exceeded'
            break

        page_html, page_count = request_page(name, i)
        if page_count == 0:
            break
        total_count += page_count

        html += page_html
        i += 1

    html += '\n<body>\n</html>\n'
    print 'Total paged items: %d' % total_count
    return html

def cmd_manifest():
    games = {}

    page_html = request_all_pages('gamesShelfMore')
    etree = parser.parse(page_html)
    page_html = request_all_pages('gamesListMore')
    list_etree = parser.parse(page_html)

    for game in etree.findall(".//div[@class='shelf_game']"):
        g = AttrDict()
        g.key = game.attrib['data-gameid']
        g.cover = THUMB +\
                     game.find(".//img[@src]").attrib['src']
        with request(ACCOUNT_AJAX_URL, args={'a': 'gamesShelfDetails',
                                        'g': g.key}) as data_request:
            game_data = json.load(data_request)
        game_element = parser.parse(game_data['details']['html'])
        g.title = game_element.find(".//h2/a").text.strip()

        # get thumbnail
        list_element = list_etree.find(".//div[@id='game_li_{0}']".\
                                                        format(g.key))
        g.background = THUMB + list_element.attrib['data-background']
        g.thumb = THUMB + list_element.find(".//img[@src]").\
                                                        attrib['src']
        g.setup, g.extra = [], []
        # get setup downloader for windows
        for dl_link in game_element.findall((".//div[@class="
                    "'win-download']/a[@class='list_game_item']")):
            g.setup.append(AttrDict(href=dl_link.attrib['href']))

        # get bonus material
        for dl_link in game_element.findall((".//div[@class="
                    "'bonus_content_list browser']/a")):
            g.extra.append(AttrDict(
                href=THUMB + dl_link.attrib['href'],
                desc=dl_link.find("./span[@class='light_un']").text)
            )
        games[g.key] = g

        # if gamecard is not None:
        #     g = games[gamecard.attrib['href'].split('/')[-1]]
        #     g.cover = THUMB + game.find(".//img[@src]").attrib['src']

    # request a zero-length range from each file to determine
    # - the total size (from the Content-Range header)
    # - the target file name (from the post-redirect URL)
    # - the chunk MD5s for setup files (from the corresponding XML)
    for g in games.values():
        for i, f in enumerate(g.setup + g.extra):
            with request(f.href, range=(0, 0)) as page:
                f.size = int(page.headers['Content-Range'].split('/')[-1])
                f.name = urlparse.urlparse(page.geturl()).path.split('/')[-1]
                if i < len(g.setup):
                    f.url = page.geturl()
        for f in g.setup:
            with request(f.url.replace('?', '.xml?'), hide=True) as page:
                etree = xml.etree.ElementTree.parse(page).getroot()
            del f['url']
            assert f.name == etree.attrib['name']
            assert f.size == int(etree.attrib['total_size'])
            f.md5 = etree.attrib['md5']
            f.part = [(int(chunk.attrib['from']),
                       int(chunk.attrib['to']),
                       chunk.text)
                      for chunk in etree.findall(".//chunk[@method='md5']")]
            assert len(f.part) == int(etree.attrib['chunks'])
            f.part.sort()

    # write all this meta-data to disk
    with open(MANIFEST, 'w') as w:
        print >>w, '# %d games' % len(games)
        pprint.pprint(games.values(), width=100, stream=w)

    # optionally download cover, background and thumbnail
    if FETCHCOVERS:
        def fetch(which, g):
            href = getattr(g, which)
            path = locate(g)
            ext = os.path.splitext(href)[-1]
            fn = '{0}{1}'.format(which, ext)
            with request(href) as page:
                path = os.path.join(path, fn)
                path_dir = os.path.dirname(path)
                if not os.path.isdir(path_dir):
                    os.mkdir(path_dir)
                with open(path, 'wb') as w:
                    w.write(page.read())
        for g in games.values():
            fetch('cover', g)
            fetch('thumb', g)
            fetch('background', g)

def cmd_list():
    # summarize the manifest
    games = load_attrdicts(MANIFEST)
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    total = 0
    for g in sorted(games, key=lambda g: g.key):
        ssz = sum(f.size for f in g.setup)
        esz = sum(f.size for f in g.extra)
        total += ssz + esz
        print u'%s (%s)' % (g.key, g.title)
        print u'    %8s in %d setup files' % (megs(ssz), len(g.setup))
        print u'    %8s in %d extras' % (megs(esz), len(g.extra))
    print u'%d games, a total of %s' % (len(games), megs(total))

def cmd_compare():
    # start the comparison from scratch
    needed(*compare({}))

def cmd_update():
    # compare only what's uncached
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    needed(*compare(valid))

def cmd_fetch(fetch_all=None):
    # start an incremental comparison
    valid = dict((v.key, v) for v in load_attrdicts(VALIDATED))
    _, games, _, _ = compare(valid)
    if fetch_all is None:
        for game in games[:]:
            if raw_input(u"Download {0}? [Y/N]".\
                    format(game.title)).upper() != 'Y':
                games.remove(game)
    elif not (all == "--all" or "-a"):
        raise RuntimeError("Unknown parameter supplied. Use --all or -a.")

    sizes, rates, errors = {}, {}, {}

    # build a list of work items
    work = Queue.PriorityQueue()
    i = -sys.maxint

    for g in games:
        for f in g.setup + g.extra:
            v = valid[(g.key, f.name)]
            path = locate(g, f, f in g.extra)
            dn = os.path.dirname(path)
            if dn and not os.path.isdir(dn):
                os.makedirs(dn)
            if os.path.isfile(path) and v.get('drop'):
                os.remove(path)
            i += 1
            for j, (start, end) in enumerate(v.need, -sys.maxint):
                # BREADTHFIRST: first chunk of all files, then second chunk...
                # else: all chunks of first file, then of second file...
                # ...after all chunks, extras in ascending size order
                if hasattr(f, 'part'):
                    prio = j if BREADTHFIRST else i
                else:
                    prio = end
                args = f, start, end, path
                work.put((prio, args))
                sz = end + 1 - start
                sizes[path] = sz + sizes.get(path, 0)

    # work item I/O loop
    def ioloop(tid, path, page, out):
        sz, t0 = True, time.time()
        while sz:
            buf = page.read(4*1024)
            t = time.time()
            out.write(buf)
            sz, dt, t0 = len(buf), t - t0, t
            with lock:
                sizes[path] -= sz
                rates.setdefault(path, []).append((tid, (sz, dt)))

    # worker thread main loop
    def worker():
        tid = threading.current_thread().ident
        while not work.empty():
            _, (f, start, end, path) = work.get()
            try:
                with open_notrunc(path) as out:
                    out.seek(start)
                    se = start, end
                    with request(f.href, range=se, hide=True) as page:
                        hdr = page.headers['Content-Range'].split()[-1]
                        assert hdr == '%d-%d/%d' % (start, end, f.size)
                        assert out.tell() == start
                        ioloop(tid, path, page, out)
                        assert out.tell() == end + 1
            except IOError, e:
                with lock:
                    print >>sys.stderr, '!', path
                    errors.setdefault(path, []).append(e)
            work.task_done()

    # detailed progress report
    def progress():
        with lock:
            left = sum(sizes.values())
            print
            for path, flowrates in sorted(rates.items()):
                flows = {}
                for tid, (sz, t) in flowrates:
                    szs, ts = flows.get(tid, (0, 0))
                    flows[tid] = sz + szs, t + ts
                bps = sum(szs/ts for szs, ts in flows.values() if ts > 0)
                print '%10s %8.1fK/s %2dx  %s' % \
                    (megs(sizes[path]), bps / 1000.0, len(flows), path)
            print megs(left), 'remaining'
            rates.clear()

    # process work items with a thread pool
    lock = threading.Lock()
    pool = []
    for i in range(CONCURRENCY):
        t = threading.Thread(target=worker)
        t.daemon = True
        t.start()
        pool.append(t)
    try:
        while any(t.is_alive() for t in pool):
            progress()
            time.sleep(1)
    except:
        with lock:
            if errors:
                print >>sys.stderr, len(errors), 'files had errors:'
            for path in errors:
                print >>sys.stderr, '!', path
                for e in errors[path]:
                    print >>sys.stderr, '\t', e
        raise

def main(args):
    arg0 = sys.argv[0]

    # check for options
    for opt in args:
        if not opt.startswith('-'):
            break
        if opt == '--help':
            print __doc__ % arg0
            return 0
        if opt == '--version':
            print 'gog-backup version', __version__
            return 0

    os.chdir(BACKUPINTO)

    # load saved cookies, if any
    try:
        cookiejar.load()
    except IOError:
        pass

    # load the path map, if any
    try:
        with open(PATHMAP, 'rU') as p:
            for line in p:
                k, v = line[:-1].split(None, 1)
                pathmap[k] = v
    except IOError:
        pass

    # dispatch to subcommand handler
    try:
        globals()['cmd_' + args[0]](*args[1:])
    except IndexError:
        print >>sys.stderr, '%s: missing command' % arg0
        return 10
    except KeyError:
        print >>sys.stderr, '%s: invalid command %r' % (arg0, args[0])
        return 10
    except TypeError, e:
        msg = e.message.replace('cmd_%s()' % args[0], repr(args[0]))
        print >>sys.stderr, '%s: %s' % (arg0, msg)
        return 10
    except RuntimeError, e:
        print >>sys.stderr, '%s: %s' % (arg0, e.message)
        return 1

    return 0

if __name__ == '__main__':
    sys.exit(main(sys.argv[1:]))
